{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Descending_Gradient.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"1ywOEpdT5Q4R"},"source":["<p><h1><b>Chapter 8: Descending Gradient</b></h1></p>\n","<p><i>\"Those who boast of their descendants have an advantage of what they owe to others\" </i></p>\n","<p> -Seneca</p>\n","<p><b> Estimating the Gradient</b></p>\n","<p> If $f$ is a variable function, its derivative in a point $x$ indicates that $f(x)$ changes when we make a slight change in $x$. Its defines as the limit of diferencial quotient:</p>"]},{"cell_type":"code","metadata":{"id":"PYgXD5Ee5Q4V"},"source":["def difference_quotient(f,x,h):\n","    return (f(x+h) - f(x))/h"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rAIyGuda5Q4W"},"source":["as $h$ approaches zero. \n","<p> To many functions is easy to calculate the derivative with precision. For example, the square function:</p>"]},{"cell_type":"code","metadata":{"id":"B5KurI0s5Q4W"},"source":["def square(x):\n","    return x**2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a2etyYcn5Q4W"},"source":["has the derivative:"]},{"cell_type":"code","metadata":{"id":"seBo5Uga5Q4X"},"source":["def derivative(x):\n","    return 2*x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MuhTJwxs5Q4X"},"source":["We can check this with the graphic:"]},{"cell_type":"code","metadata":{"id":"TwYtzHt_5Q4X"},"source":["import matplotlib.pyplot as plt\n","from functools import partial\n","#you can acknowledge this function by the following use"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jhl0-IBI5Q4X"},"source":["derivative_estimate = partial(difference_quotient, square, h=0.00001)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KiMOPh4g5Q4Y"},"source":["x = range(-10,10)\n","y_der = list(map(derivative,x))\n","y_est = list(map(derivative_estimate,x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BI3EHVMP5Q4Y","outputId":"f55e07c0-85fa-4ef7-834c-6fb1440803cd"},"source":["plt.title(\"Actual Derivatives vs. Estimates\")\n","plt.plot(x, y_der, 'rx',label=\"Actual\")\n","plt.plot(x, y_est, 'b+', label='Estimative')\n","plt.legend(loc=9)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"bBYUAjOn5Q4Z"},"source":["<p> We can calculate the parcial derivative of the i-th as this: </p>"]},{"cell_type":"code","metadata":{"id":"9SOIa6JG5Q4Z"},"source":["def partial_difference_quotient(f,v,i,h):\n","    w = [v_j + (h if j == i else 0)\n","         for j, v_j in enumarate(v)]\n","    return (f(w) - f(v))/h"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X8jnj5uc5Q4Z"},"source":["And then, estimate the gradient in the same way:"]},{"cell_type":"code","metadata":{"id":"jO7EElnu5Q4Z"},"source":["def estimate_gradient(f, v, h=0.00001):\n","    return [partial_difference_quotient(f, v, i, h)\n","           for i, _ in enumerate(v)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AeKS-noy5Q4Z"},"source":[" <p><b> Using the Gradient</b></p>\n"," <p> Its easy to see that <tt>sum_of_squares_gradient</tt> has its minimum when $v$ is a vector of zeros. But, imagine that if we didn't know. We are going to use gradient to find the minal between every tridimensional vectors. We will pick a random initial point and move small steps in the opposite direction of the gradient, until we find a point where the gradient is small enough.</p>\n"," <p> We are going to use derivative to calculate the size of the step. The bigger the derivative, the bigger the step.</p>\n"," <p> $x_{n+1}^k = x_n^k + \\alpha\\cdot 2\\cdot x_n^k$</p>\n"," <p> where:</p>\n"," <p> $x_n =  \\begin{bmatrix}\n","x_n^1\\\\\n","x_n^2\\\\\n","...\\\\\n","x_n^k\n","\\end{bmatrix}$ is our vector and $\\alpha \\equiv -0.01$ is our coeficient. </p>"]},{"cell_type":"code","metadata":{"id":"O41_ox8P5Q4a"},"source":["import random, math"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KL1jwK395Q4a"},"source":["def step(v, direction, step_size):\n","    return [v_i + step_size * direction_i\n","           for v_i, direction_i in zip(v, direction)]\n","\n","def sum_of_squares_gradient(v):\n","    return [2*v_i for v_i in v]\n","\n","def distance(v, next_v):\n","    dist = 0\n","    for v_i, next_v_i in zip(v, next_v):\n","        dist += (v_i - next_v_i)**2\n","    return math.sqrt(dist)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GhQcwEEE5Q4a"},"source":["v = [random.randint(-10,10) for i in range(3)]\n","tolerance = 0.0000001"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_yVxUB35Q4a","outputId":"ecb5817b-7744-4eeb-b04f-6b1ee92b9585"},"source":["while True:\n","    gradient = sum_of_squares_gradient(v)\n","    next_v = step(v,gradient,-0.01)\n","    if distance(next_v, v) < tolerance:\n","        break\n","    v = next_v\n","print(v)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-2.2215127272775794e-06, 3.5544203636441233e-06, 2.6658152727330933e-06]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"091sVPCd5Q4a"},"source":["<p><b> Choosing the Size of the Next Step</b></p>\n","<p> Although the logic begind the direction of gradient is clear, the step isn't. Choosing the size of the next step is more an art than science. The most popular options are:</p>\n","<ul>\n","    <li> Fixed size step</li>\n","    <li> Decrease gradually the size of the step each time</li>\n","    <li> For each step, choose a different size of the step to minimize the value of the function</li>\n","</ul>\n","<p> The last one seems to be the best, but has a high computational cost. We can approximate the step size by picking different pre-determineted step sizes. </p>"]},{"cell_type":"code","metadata":{"id":"4yrIV6g55Q4b"},"source":["step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"78S-4x_g5Q4b"},"source":["<p> Its possible that some step sizes will result in invalid inputs for our function. So, you will nedd a \"safe application\" function that will return infinite to invalid inputs.</p>"]},{"cell_type":"code","metadata":{"id":"Q_GDuYeS5Q4b"},"source":["def safe(f):\n","    def safe_f(*args, **kwargs):\n","        try:\n","            return f(*args, **kwargs)\n","        except:\n","            return float('inf')\n","    return safe_f"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7AS_Owdp5Q4b"},"source":["<p> <b>Putting Everything Together</b></p>\n","<p> Given a function -<tt>target_fn</tt>, and its gradient - <tt>gradient_fn</tt>, that we want to minimize, sometimes the function could represent errors in a module with a function of its parameters, and maybe we want to find the parameters that produces the smallest possible errors.</p>\n","<p> Generally, we would choose the parameters that we are going to call $\\theta$ (<tt>theta</tt>). When we are working with linear functions, the <tt> theta </tt> parameters of a hypotesis function would look like that:</p>\n","<p> $h_{\\theta}(x) = \\theta_0 + \\theta_1 x$ </p>\n","<p> We are going to call a <tt> minimize_batch</tt> that will, for each step of the bradient, consider whole sets of data. </p>"]},{"cell_type":"code","metadata":{"id":"lj6HKEaU5Q4b"},"source":["def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.0000001):\n","    \n","    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n","    \n","    theta = theta_0\n","    target_fn = safe(target_fn)\n","    value = target_fn(theta)\n","    \n","    while True:\n","        gradient = gradient_fn(theta)\n","        next_theta = [step(theta, gradient, -step_size)\n","                       for step_size in step_sizes]\n","        next_theta = min(next_thetas, key=target_fn)\n","        next_value = target_fn(next_theta)\n","        \n","        if abs(value - next_value) < tolerance:\n","            return theta\n","        else:\n","            theta, value = next_theta, next_value"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sU8rOS-95Q4c"},"source":["Sometimes we want to maxime a function and we can do it by minimizing its negative."]},{"cell_type":"code","metadata":{"id":"kMPTa5245Q4c"},"source":["def negate(f):\n","    #returns a function that, for every input, x returns -f(x)\n","    return lambda *args, **kwargs: -f(*args, **kwargs)\n","    \n","def negate_all(f):\n","    #returns a inversed signal list of numbers\n","    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5nbCL_Gq5Q4c"},"source":["def maximize_batch(target_fn, gradient_fn, thetha_0, tolerance=0.000001):\n","    return minimize_batch(negate(target_fn),\n","                         negate_all(gradient_fn),\n","                         theta_0,\n","                         tolerance)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rp1bmi_C5Q4c"},"source":["<p><b> Estocastic Descendent Gradient</b></p>\n","<p> We are frequently going to use the descendent gradient to pick teh parameters of a model in the way to minimize the error. Generally, as seen before, the error of the whole set of data is the sum of each individual error.</p>\n","<p> For example, the cost for the whole set of data to a bidimensional linear regression is:  </p>\n","<p> $Cost = \\Sigma_{i=1}^n (\\theta_1 x_i + \\theta_0 - y_i)^2$</p>\n","<p> and that is what we want to minimize.</p>\n","<p> When this is the case, we can apply a technique called \"Estocastic Descendent Gradient\". that computates the gradient (and takes a step) one point at a time. It circulates over our data repeatedly until find a stop point.</p>\n","<p> For every cicle, we want to iterate over our data in a random order. </p>"]},{"cell_type":"code","metadata":{"id":"MOh2o31_5Q4d"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6wFYiRgA5Q4d"},"source":["def in_random_order(data):\n","    indexes = [i for i,_ in enumerate(data)]\n","    random.shuffle(intexes)\n","    for i in indexes:\n","        yield data[i]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VZHRGAlu5Q4f"},"source":["We are going to give a gradient step for each point of the data. This allows us to circulates nearby a minimun forever and, eventually, stop."]},{"cell_type":"code","metadata":{"id":"ugCc1P1s5Q4f"},"source":["def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0 = 0.01):\n","    \n","    data = zip(x,y)\n","    theta = theta_0\n","    alpha = alpha_0\n","    min_theta, min_value = None, float(\"inf\")\n","    iterations_with_no_improvement = 0\n","    \n","    while interactions_with_no_improvement < 100:\n","        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data)\n","        \n","        if value < min_value:\n","            min_theta, min_value = theta, value\n","            iterations_with_no_improvement = 0\n","            alpha = alpha_0\n","        else:\n","            iterations_with_no_improvement += 1\n","            alpha *= 0.9\n","        for x_i, y_i in in_random_order(data):\n","            gradient_i = gradient_fn(x_i,y_i, theta)\n","            theta = np.subtract(theta, np.array(gradient_i)*alpha)\n","            \n","    return min_theta"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A2dHO-Rc5Q4g"},"source":["The stocastic version is tipically faster than the batch version. Naturally, we would want a version that maximizes the same way:"]},{"cell_type":"code","metadata":{"id":"2hbRp_W55Q4g"},"source":["def maximize_stocastic(target_fn, gradient_fn, x, y, theta_0, alpha_0 = 0.01):\n","    return minimize_stocastic(negate(target_fn),\n","                             negate_all(gradient_fn),\n","                             x,y,theta_0, alpha_0)"],"execution_count":null,"outputs":[]}]}